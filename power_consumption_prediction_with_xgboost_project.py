# -*- coding: utf-8 -*-
"""Power_consumption_prediction_with_xgboost_PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D83M_WvImzHNHLgcp1cz5BJMLcFm58Rj
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb
from xgboost import plot_importance, plot_tree
from sklearn.metrics import mean_squared_error, mean_absolute_error
plt.style.use('fivethirtyeight')

"""# Data
The data we will be using is hourly power consumption data from UCI machine learning repository. Energy consumtion has some unique charachteristics. It will be interesting to see how prophet picks them up.

Pulling the `Global active power` which has data from December 2006 and November 2010 (47 months).
"""

data = pd.read_csv('/content/household_power_consumption.csv', index_col=[0], parse_dates=[0])

color_pal = ["#F8766D", "#D39200", "#93AA00", "#00BA38", "#00C19F", "#00B9E3", "#619CFF", "#DB72FB"]
_ = data.plot(style='.', figsize=(15,5), color=color_pal[0], title='Global_active_power_at_every_hour')

"""# Train/Test Split
Cut off the data after 2015 to use as our validation set.
"""

split_date = '26-nov-2009'
data_train = data.loc[data.index <= split_date].copy()
data_test = data.loc[data.index > split_date].copy()

_ = data_test \
    .rename(columns={'Global_active_power': 'TEST SET'}) \
    .join(data_train.rename(columns={'Global_active_power': 'TRAINING SET'}), how='outer') \
    .plot(figsize=(15,5), title='test train spilit', style='.')

"""# Create Time Series Features"""

def create_features(df, label=None):
    """
    Creates time series features from datetime index
    """
    df['date'] = df.index
    df['hour'] = df['date'].dt.hour
    df['dayofweek'] = df['date'].dt.dayofweek
    df['quarter'] = df['date'].dt.quarter
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    df['dayofyear'] = df['date'].dt.dayofyear
    df['dayofmonth'] = df['date'].dt.day
    df['weekofyear'] = df['date'].dt.weekofyear
    
    X = df[['hour','dayofweek','quarter','month','year',
           'dayofyear','dayofmonth','weekofyear']]
    if label:
        y = df[label]
        return X, y
    return X

X_train, y_train = create_features(data_train, label='Global_active_power')
X_test, y_test = create_features(data_test, label='Global_active_power')

"""# Create XGBoost Model"""

reg = xgb.XGBRegressor(n_estimators=1000)
reg.fit(X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        early_stopping_rounds=50,
       verbose=False) # Change verbose to True if you want to see it train

"""## Feature Importances
Feature importance is a great way to get a general idea about which features the model is relying on most to make the prediction. This is a metric that simply sums up how many times each feature is split on.

We can see that the day of year was most commonly used to split trees, while hour and year came in next. Quarter has low importance due to the fact that it could be created by different dayofyear splits.
"""

_ = plot_importance(reg, height=0.9)

"""# Forecast on Test Set"""

data_test['KW_Prediction'] = reg.predict(X_test)
data_all = pd.concat([data_test, data_train], sort=False)

_ = data_all[['Global_active_power','KW_Prediction']].plot(figsize=(20, 5))

"""# Look at first month of predictions (winters)"""

# Plot the forecast with the actuals
f, ax = plt.subplots(1)
f.set_figheight(5)
f.set_figwidth(15)
_ = data_all[['Global_active_power','KW_Prediction']].plot(ax=ax,
                                              style=['-','-'])
ax.set_xbound(lower='01-jan-2010', upper='01-feb-2010')
ax.set_ylim(0, 500)
plot = plt.suptitle('January 2010 || Forecast vs Actuals')

"""# Look at first week of predictions"""

# Plot the forecast with the actuals
f, ax = plt.subplots(1)
f.set_figheight(5)
f.set_figwidth(15)
_ = data_all[['Global_active_power','KW_Prediction']].plot(ax=ax,
                                              style=['-','-'])
ax.set_xbound(lower='01-jan-2010', upper='08-jan-2010')
ax.set_ylim(0, 500)
plot = plt.suptitle('First Week of January 2010 || Forecast vs Actuals')

"""# predictions of month (summer)"""

# Plot the forecast with the actuals
f, ax = plt.subplots(1)
f.set_figheight(5)
f.set_figwidth(15)
_ = data_all[['Global_active_power','KW_Prediction']].plot(ax=ax,
                                              style=['-','-'])
ax.set_xbound(lower='01-jun-2010', upper='01-jul-2010')
ax.set_ylim(0, 500)
plot = plt.suptitle('June 2010 || Forecast vs Actuals')

f, ax = plt.subplots(1)
f.set_figheight(5)
f.set_figwidth(15)
_ = data_all[['Global_active_power','KW_Prediction']].plot(ax=ax,
                                              style=['-','-'])
ax.set_ylim(0, 500)
ax.set_xbound(lower='01-jun-2010', upper='08-jun-2010')
plot = plt.suptitle('First Week of June 2010 Forecast vs Actuals')

"""# Error Metrics On Test Set
Our RMSE error is 1404.5  
Our MAE error is 28  
Our MAPE error is 64%
"""

mean_squared_error(y_true=data_test['Global_active_power'],y_pred=data_test['KW_Prediction'])

mean_absolute_error(y_true=data_test['Global_active_power'],y_pred=data_test['KW_Prediction'])

"""I like using mean absolute percent error because it gives an easy to interperate percentage showing how off the predictions are.
MAPE isn't included in sklearn so we need to use a custom function.
"""

def mean_absolute_percentage_error(y_true, y_pred): 
    """Calculates MAPE given y_true and y_pred"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mean_absolute_percentage_error(y_true=data_test['Global_active_power'],y_pred=data_test['KW_Prediction'])

"""# Look at Worst and Best Predicted Days"""

data_test['error'] = data_test['Global_active_power'] - data_test['KW_Prediction']
data_test['abs_error'] = data_test['error'].apply(np.abs)
error_by_day = data_test.groupby(['year','month','dayofmonth']) \
    .mean()[['Global_active_power','KW_Prediction','error','abs_error']]

# Over forecasted days
error_by_day.sort_values('error', ascending=True).head(10)

"""Notice anything about the over forecasted days? 
- #1 worst day - november 21st, 2010 - is a sunday. 
- #2 worst day - December 20th, 2009 - is a sunday
- #3 worst day - January  31st, 2010 - is also a sunday.   
Looks like our model may benefit from adding a sunday indicator.
"""

# Worst absolute predicted days
error_by_day.sort_values('abs_error', ascending=False).head(10)

"""The best predicted days seem to be a lot of july and august (not many holidays and mild weather)"""

# Best predicted days
error_by_day.sort_values('abs_error', ascending=True).head(10)

"""# Plotting some best/worst predicted days"""

f, ax = plt.subplots(1)
f.set_figheight(5)
f.set_figwidth(10)
_ = data_all[['Global_active_power','KW_Prediction']].plot(ax=ax,
                                              style=['-','.'])
ax.set_ylim(0, 500)
ax.set_xbound(lower='21-nov-2010', upper='22-nov-2010')
plot = plt.suptitle('nov 21, 2010 - Worst Predicted Day')

"""This one is pretty impressive. SPOT ON!"""

f, ax = plt.subplots(1)
f.set_figheight(5)
f.set_figwidth(10)
_ = data_all[['Global_active_power','KW_Prediction']].plot(ax=ax,
                                              style=['-','.'])
ax.set_ylim(0, 500)
ax.set_xbound(lower='06-jul-2010', upper='07-jul-2010')
plot = plt.suptitle('jul 06, 2010 - Best Predicted Day')